{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import imutils\n",
    "from imutils import paths\n",
    "import os\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting letters from each captcha for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOLVED_CAPTCHA_FOLDER = \"solved-captchas\"\n",
    "OUTPUT_FOLDER = \"extracted_letters\"\n",
    "\n",
    "\n",
    "# Get the path of all the solved captcha images\n",
    "solved_captchas = glob.glob(os.path.join(SOLVED_CAPTCHA_FOLDER, \"*\"))\n",
    "counts = {}\n",
    "\n",
    "# loop over the image paths\n",
    "for (i, captcha) in enumerate(solved_captchas):\n",
    "    print(\"processing image {}/{}\".format(i + 1, len(solved_captchas)))\n",
    "\n",
    "    \n",
    "    # grab the base filename as the text\n",
    "    filename = os.path.basename(captcha)\n",
    "    captcha_text = os.path.splitext(filename)[0]\n",
    "\n",
    "    # Load the image and convert it to grayscale\n",
    "    image = cv2.imread(captcha)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Add some extra padding around the image\n",
    "    gray = cv2.copyMakeBorder(gray, 8, 8, 8, 8, cv2.BORDER_REPLICATE)\n",
    "\n",
    "    # applying threshold\n",
    "    thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV, cv2.THRESH_OTSU)[1]\n",
    "\n",
    "     # finding the contours\n",
    "    contours, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    # creating empty list for holding the coordinates of the letters\n",
    "    letter_image_regions = []\n",
    "\n",
    "    # Now we will loop through each of the contours and extract the letter\n",
    "    for contour in contours:\n",
    "        # Get the rectangle that contains the contour\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        \n",
    "        # checking if any counter is too wide\n",
    "        # if countour is too wide then there could be two letters joined together or are very close to each other\n",
    "        if w / h > 1.25:\n",
    "            # Split it in half into two letter regions\n",
    "            half_width = int(w / 2)\n",
    "            letter_image_regions.append((x, y, half_width, h))\n",
    "            letter_image_regions.append((x + half_width, y, half_width, h))\n",
    "        else:\n",
    "            \n",
    "            letter_image_regions.append((x, y, w, h))\n",
    "            \n",
    "\n",
    "    \n",
    "    # Sort the detected letter images based on the x coordinate to make sure\n",
    "    # we get them from left-to-right so that we match the right image with the right letter\n",
    "    letter_image_regions = sorted(letter_image_regions, key=lambda x: x[0])\n",
    "    \n",
    "    # Save each letter as a single image\n",
    "    for letter_bounding_box, letter_text in zip(letter_image_regions, captcha_text):\n",
    "        # Grab the coordinates of the letter in the image\n",
    "        x, y, w, h = letter_bounding_box\n",
    "\n",
    "        # Extract the letter from the original image with a 2-pixel margin around the edge\n",
    "        letter_image = gray[y - 2:y + h + 2, x - 2:x + w + 2]\n",
    "\n",
    "\n",
    "        # Get the folder to save the image in\n",
    "        save_path = os.path.join(OUTPUT_FOLDER, letter_text)\n",
    "\n",
    "        # creating different output folder for storing different letters\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "        # write the letter image to a file\n",
    "        count = counts.get(letter_text, 1)\n",
    "        p = os.path.join(save_path, \"{}.png\".format(str(count)))\n",
    "        cv2.imwrite(p, letter_image)\n",
    "\n",
    "        # increment the count\n",
    "        counts[letter_text] = count + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4695, 30, 30, 1) (4695,)\n"
     ]
    }
   ],
   "source": [
    "letter_folder = 'extracted_letters'\n",
    "\n",
    "#creating empty lists for storing image data and labels\n",
    "data = []\n",
    "labels = []\n",
    "for image in paths.list_images(letter_folder):\n",
    "    img = cv2.imread(image)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.resize(img, (30,30))\n",
    "    \n",
    "    # adding a 3rd dimension to the image\n",
    "    img = np.expand_dims(img, axis = 2)\n",
    "    \n",
    "    #grabing the name of the letter based on the folder it is present in\n",
    "    label = image.split(os.path.sep)[-2]\n",
    "    \n",
    "    # appending to the empty lists\n",
    "    data.append(img)\n",
    "    labels.append(label)\n",
    "\n",
    "#converting data and labels to np array\n",
    "data = np.array(data, dtype = \"float\")\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(data.shape, labels.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling the values of  data between 0 and 1\n",
    "data = data/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3756, 30, 30, 1) (939, 30, 30, 1) (3756,) (939,)\n"
     ]
    }
   ],
   "source": [
    "# Split the training data into separate train and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "(train_x, val_x, train_y, val_y) = train_test_split(data, labels, test_size=0.2, random_state=0)\n",
    "print(train_x.shape, val_x.shape, train_y.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3756, 9) (939, 9)\n"
     ]
    }
   ],
   "source": [
    "# one hot encoding our target variable 'labels'\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "lb = LabelBinarizer().fit(train_y)\n",
    "train_y = lb.transform(train_y)\n",
    "val_y = lb.transform(val_y)\n",
    "print(train_y.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 20)        520       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 15, 20)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 15, 50)        25050     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 50)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2450)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               313728    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 1161      \n",
      "=================================================================\n",
      "Total params: 340,459\n",
      "Trainable params: 340,459\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(20, (5, 5), padding=\"same\", input_shape=(30, 30, 1), activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Conv2D(50, (5, 5), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(9, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using early stoping for avoiding overfitting\n",
    "estop = EarlyStopping(patience=10, mode='min', min_delta=0.001, monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3756 samples, validate on 939 samples\n",
      "Epoch 1/50\n",
      "3756/3756 [==============================] - 4s 1ms/step - loss: 0.3539 - acc: 0.8972 - val_loss: 0.0480 - val_acc: 0.9968\n",
      "Epoch 2/50\n",
      "3756/3756 [==============================] - 1s 250us/step - loss: 0.0197 - acc: 0.9981 - val_loss: 0.0504 - val_acc: 0.9968\n",
      "Epoch 3/50\n",
      "3756/3756 [==============================] - 1s 255us/step - loss: 0.0213 - acc: 0.9979 - val_loss: 0.0515 - val_acc: 0.9968\n",
      "Epoch 4/50\n",
      "3756/3756 [==============================] - 1s 245us/step - loss: 0.0268 - acc: 0.9976 - val_loss: 0.0444 - val_acc: 0.9968\n",
      "Epoch 5/50\n",
      "3756/3756 [==============================] - 1s 249us/step - loss: 0.0193 - acc: 0.9981 - val_loss: 0.0449 - val_acc: 0.9968\n",
      "Epoch 6/50\n",
      "3756/3756 [==============================] - 1s 253us/step - loss: 0.0180 - acc: 0.9987 - val_loss: 0.0515 - val_acc: 0.9968\n",
      "Epoch 7/50\n",
      "3756/3756 [==============================] - 1s 250us/step - loss: 0.0189 - acc: 0.9987 - val_loss: 0.0473 - val_acc: 0.9968\n",
      "Epoch 8/50\n",
      "3756/3756 [==============================] - 1s 246us/step - loss: 0.0140 - acc: 0.9987 - val_loss: 0.0497 - val_acc: 0.9968\n",
      "Epoch 9/50\n",
      "3756/3756 [==============================] - 1s 246us/step - loss: 0.0153 - acc: 0.9987 - val_loss: 0.0482 - val_acc: 0.9968\n",
      "Epoch 10/50\n",
      "3756/3756 [==============================] - 1s 252us/step - loss: 0.0154 - acc: 0.9987 - val_loss: 0.0448 - val_acc: 0.9968\n",
      "Epoch 11/50\n",
      "3756/3756 [==============================] - 1s 251us/step - loss: 0.0158 - acc: 0.9987 - val_loss: 0.0512 - val_acc: 0.9968\n",
      "Epoch 12/50\n",
      "3756/3756 [==============================] - 1s 256us/step - loss: 0.0157 - acc: 0.9987 - val_loss: 0.0519 - val_acc: 0.9968\n",
      "Epoch 13/50\n",
      "3756/3756 [==============================] - 1s 253us/step - loss: 0.0143 - acc: 0.9987 - val_loss: 0.0516 - val_acc: 0.9968\n",
      "Epoch 14/50\n",
      "3756/3756 [==============================] - 1s 255us/step - loss: 0.0143 - acc: 0.9987 - val_loss: 0.0495 - val_acc: 0.9968\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x26b32bbd5c0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, validation_data=(val_x, val_y), batch_size=32, epochs=50, verbose=1, callbacks = [estop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model on an unseen captcha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CAPTCHA text is: 66372\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26b332a0908>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAACnCAYAAAABvqdmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAE5VJREFUeJzt3X2wXHV9x/H3xwSwiFxCJCFNmAZruJU6BTQjD+lAigIRmARmpEIdSKeZyTi204B0IMDIiKJCi0LrWOUKQmBSAo0GMhmQRh6CKBOTgEJCIIEIGkkJIISnsRr49o89Z7PZu/fu856zJ58Xk9nztPd893f3fvme3zm/cxQRmJlZ/3tP1gGYmVlnOKGbmRWEE7qZWUE4oZuZFYQTuplZQTihm5kVhBO6mVlBtJXQJc2S9LSkZyQt7FRQZmbWPLU6sEjSGGATcBKwFVgDnBMRT3YuPDMza9TYNt77ceCZiNgCIGkJMAcYMaEPDAzEwQcf3MYu+9um929q6/2HvXFYhyIpnnbbtha3t+XFpk2bXo6Ig+pt105Cnwz8pmJ+K3B09UaS5gPzASZOnMjQ0FAbu+xPM0+Y2ZGfM7Rqz2u7Vri9rWhmzpz5fCPbtdOHrhrLhvXfRMRQREyPiOkDAwNt7M7MzEbTTkLfChxSMT8FeKG9cMzMrFXtdLmsAaZJOhT4LXA28HcdiWoP9OCqB4HRuwsq16Xb23CjtU0j3TFuW+tXLSf0iNgp6Z+Ae4ExwPcjYkPHIjMzs6a0U6ETEXcDd3colj1SdTVYOV9dTRapcszbZ8t6/2ad4JGiZmYF0VaFbqNrpL+2yH26zVw+WGvbTn3uZn8P/dreZq7QzcwKwgndzKwg3OXSB9LugH7pCujUSM2stNrenfrcWf2ei9z9t6dwhW5mVhCu0PtIkU7cNTOQqt3POtqloO3qxtFIL4/Imo2/344W9zSu0M3MCsIVesbaHabeT2p91tEq9W5UgdU/s9027mb1303txlqko8UicYVuZlYQrtAzUrSqplODqKB/26aZuLOq5lu9kqUX5zqsfa7QzcwKwgndzKwg3OViudPKIXyvTqr2QlZxj7bfRi4ztezVrdAlfV/SdknrK5YdKGmlpM3J67juhmlmZvUoYthjQHffQDoeeBO4JSI+kiz7V+B3EXGVpIXAuIi4uN7OBgcHo1sPiT7hhBO68nM7QTUfv1oSwx/D2vb7sjBarO3qZhtl1cZZ/27T/Te6r0Z+v3n7TmZl1apVHf+ZM2fOXBcR0+ttV7dCj4iHgN9VLZ4DLEqmFwFnNB2hmZl1VKt96BMjYhtARGyTNGGkDSXNB+YDTJw4scXdFVc3K9u8abXSblUe2zbryrwb+3Jlnh9dv8olIoYiYnpETB8YGOj27szM9litJvQXJU0CSF63dy4kMzNrRasJfTkwN5meC9zVmXCKKZL/OvVz+u0Qt99i7rd4u0UV/1l/aOSyxduAR4BBSVslzQOuAk6StBk4KZk3M7MM1T0pGhHnjLDqEx2OpfBqVX15OUnWr5pt00be32n99jv2JYr9y0P/zcwKwkP/M1aUSqfyc1RXeM0OYulkLNY4V+b9zxW6mVlBOKGbmRWEu1x6KOvLv7I+XO7V589LO/fDydBG2yov8droXKGbmRWEK/SM9LLi6XXF2kiF2szPyer9zcr6yKAZPgFaTK7QzcwKwhV6TrzyyisAfOUrXykvu+222wB49dVXAfjYxz5WXvfFL34RgFNPPbVXITatusLLqk+58v7U3/jGNwC4//77AfjDH/4AwPjx48vbzJ49G4Avf/nLQPN3Cc1z33kzRxHuX+8/rtDNzArCCd3MrCDc5ZKxt99+G9j1CL233nqrvO7ee+8F4MgjjwTgySefLK9Lu2by3OVSLatD87SbBeDCCy8E4M477wTgPe8p1TTr1q0rbzNjxgwA1qxZA8Cjjz7akzj7Va9HAtvIXKGbmRWEK/SMpdXjhg0bALjjjjvK69LKPHX44YeXp9MTpnmWl8v4li9fXnebI444ojy9c+dOALZs2dLS/qov28xT5dqpS0pr/UzLXiP3Qz9E0gOSNkraIGlBsvxASSslbU5ex3U/XDMzG0kjFfpO4MKIeFTS+4F1klYCfw/cFxFXSVoILAQu7l6oxbRs2bLd5ufMmZNRJN2TxwE+7777LgDr168H4Morryyv23vvvQG4/vrr24ojz5VrnmOz1tWt0CNiW0Q8mky/AWwEJgNzgEXJZouAM7oVpJmZ1dfUSVFJU4GjgNXAxIjYBqWkD0wY4T3zJa2VtHbHjh3tRWtmZiNq+KSopP2AHwDnR8TrUoOjyCKGgCGAwcFBH+dVeeqpp3abv/XWW8vT3/rWtwDYtGkTsPslit/73vcAGDeuP09dVI+MrTzJWz0yNh0VC+1fpjnS9/b4448vT2/evBmAyZMnt7aPnNztsRFZxeoun+5oqEKXtBelZL44In6YLH5R0qRk/SRge3dCNDOzRtSt0FUqaW4ENkbENytWLQfmAlclr3d1JcKCS+8lknrooYfK0w8//PBuy0477bRh71+6dGkXo+usdBAVDB9IlQ6iguEDqSrvb9NuhR5RqgzTk6KrV68G4KKLLipvc9xxxwGwZMkSAI499tjW9pXDk8GjyfqB2da+RrpcZgDnAk9I+kWy7FJKifwOSfOAXwNndSdEMzNrhNKKpRcGBwdjaGioKz87rfjyrLI6Sauhgw46CICXX34ZgHfeeae8TTosPa0mx4wZU163zz77APD73/++4f1mVTGm+62stC+//HJg10Cqs87qXD3Qyud96aWXytMTJpTO70+bNg3YdQ6j0f2myoN4GjzfNJJG/0Zb+dy1vpMrVqwAdr9lwk9/+lMA9t13XwDOO++88rqvfe1rAOy3335dibHfVN7ds1Nmzpy5LiKm19vOQ//NzArCQ/8zlvYJ33LLLQC8+eab5XX7778/sKtSrzQwMNCD6DqrehAV5GcgVa32TI+a2tVohZ0ebaVHBq323bcrveKocmDVhz70IQCuvvpqAC699NLyujfeeAOAm266qVch2ghcoZuZFYQTuplZQbjLJWMXX1y6/U16mPulL32pvO6aa64Bdj0urdLnP//57gfXYdWDqGDXQKp0EBUMH0iVDqKC1gZSnX766eXptL3Te56no5fPP//8Ye+rjKkX0u6M9ATttdde29P9pxYvXjziurT9Krtc0u+uu1yy5wrdzKwgXKFnLL3H+cqVKwG44IILyuvSu/6lDylOH1oMuw+H7xfVg6hg16CpdBBV5bJODaT63Oc+V55OL5d85JFHdtvm6KOPLk/fc889AMyaNavpfTXr+eefL09//etfB3ZVwa3eeqCbav0O0++pZc8VuplZQbhCz4l0YFSRn19Z2f+dXhKY9rtWXppZXRmnA11aVdmHXjmdB5VHZOmgnfS5p3l08803D1tWOcjIsuUK3cysIJzQzcwKwl0u1jOVd0qsHhmbjoqF4SNj+3FUbD0/+clPgN1Hz6aXrFa2RV6k96j/6le/CsDUqVPL66644oosQrIaXKGbmRWEK3TrmfRyPBg+kCodRAXDB1L14yCqehYsWADAXnvtVV6Wt89ZeSfPc889F9h19FR53/7x48f3NjAbUd0KXdJ7Jf1c0i8lbZB0RbL8UEmrJW2WdLskX4xqZpahRir0/wNOjIg3k0fRPSzpHuALwLURsUTSd4F5wHe6GKv1uXQQFQwfSFU5OKV6IFU/DqIayd133w3AY489BsCZZ55ZXpfeGz9r6XmN9MlNAIcddhgAzz77LABjx/rgPo/qVuhRkt7Tda/kXwAnAumwvUXAGV2J0MzMGtLoQ6LHJI+f2w6sBJ4FXouInckmW4Ga45QlzZe0VtLa9EZIZmbWeQ0dN0XEO8CRkg4AlgEfrrXZCO8dAoag9Ai6FuMsnD39Ybm9GBmbxza+4YYbdpv/zGc+k1EkIzvllFMAeOKJJ8rL0vvbuKsl35q6bDEiXgMeBI4BDpCU/nanAC90NjQzM2tG3f/dSjoI+GNEvCbpT4BPAlcDDwCfBpYAc4G7uhloERT5wbh5lFV7j3Zk8OMf/3i3+ZNPPrnb4TTtZz/72bBlU6ZMqfu+Xj5w3mpr5PhpErBI0hhKFf0dEbFC0pPAEklXAo8BN3YxTjMzq6NuQo+Ix4GjaizfAny8G0GZFdXrr7+edQh1udLuXx76b2ZWEE7oZmYF4YRuZlYQTuhmZgXhUQLWdXkc4GNWRK7QzcwKwhW6dY0HUpn1lit0M7OCcIVu1iX9du6g3+K14Vyhm5kVhBO6mVlBuMvFrMP66WRwP8Vq9blCNzMrCCd0M7OCaDihJ88VfUzSimT+UEmrJW2WdLukvev9DDMz655mKvQFwMaK+auBayNiGvAqMK+TgZmZWXMaSuiSpgCnATck8wJOBJYmmywCzuhGgGZm1phGK/TrgIuAd5P58cBrEbEzmd8KTK71RknzJa2VtHbHjh1tBWtmZiOrm9AlnQ5sj4h1lYtrbFrz+qeIGIqI6RExfWBgoMUwzcysnkauQ58BzJZ0KvBeYH9KFfsBksYmVfoU4IXuhWlmZvXUrdAj4pKImBIRU4Gzgfsj4rPAA8Cnk83mAnd1LUozM6urnevQLwa+IOkZSn3qN3YmJDMza0VTQ/8j4kHgwWR6C/DxzodkZmat8EhRM7OCcEI3MysIJ3Qzs4JwQjczKwgndDOzgnBCNzMrCCd0M7OCcEI3MysIJ3Qzs4JwQjczKwgndDOzgnBCNzMrCCd0M7OCaOpui3m2atWqrEMwM8tUQwld0nPAG8A7wM6ImC7pQOB2YCrwHPC3EfFqd8I0M7N6muly+ZuIODIipifzC4H7ImIacF8yb2ZmGWmnD30OsCiZXgSc0X44ZmbWqkYTegD/I2mdpPnJsokRsQ0geZ1Q642S5ktaK2ntjh072o/YzMxqavSk6IyIeEHSBGClpKca3UFEDAFDAIODg9FCjGZm1oCGKvSIeCF53Q4so/Qs0RclTQJIXrd3K0gzM6uvbkKX9D5J70+ngZOB9cByYG6y2Vzgrm4FaWZm9TXS5TIRWCYp3f6/IuJHktYAd0iaB/waOKt7YZqZWT11E3pEbAGOqLH8FeAT3QjKzMya56H/ZmYF4YRuZlYQTuhmZgXhhG5mVhBO6GZmBeGEbmZWEE7oZmYF4YRuZlYQTuhmZgXhhG5mVhBO6GZmBaGI3t2iXNJLwFvAyz3baed8AMfdS/0Ydz/GDI6711qJ+88i4qB6G/U0oQNIWlvxXNK+4bh7qx/j7seYwXH3WjfjdpeLmVlBOKGbmRVEFgl9KIN9doLj7q1+jLsfYwbH3Wtdi7vnfehmZtYd7nIxMysIJ3Qzs4LoWUKXNEvS05KekbSwV/ttlqRDJD0gaaOkDZIWJMsPlLRS0ubkdVzWsdYiaYykxyStSOYPlbQ6ift2SXtnHWM1SQdIWirpqaTdj+2H9pZ0QfIdWS/pNknvzWN7S/q+pO2S1lcsq9m+KvmP5O/0cUkfzVnc/5Z8Tx6XtEzSARXrLkniflrSKdlEXTvuinX/IikkfSCZ72h79yShSxoDfBv4FHA4cI6kw3ux7xbsBC6MiA8DxwD/mMS6ELgvIqYB9yXzebQA2FgxfzVwbRL3q8C8TKIa3b8DP4qIv6D0QPKN5Ly9JU0G/hmYHhEfAcYAZ5PP9r4ZmFW1bKT2/RQwLfk3H/hOj2Ks5WaGx70S+EhE/BWwCbgEIPkbPRv4y+Q9/5nknSzczPC4kXQIcBLw64rFnW3viOj6P+BY4N6K+UuAS3qx7w7EflfyS3gamJQsmwQ8nXVsNWKdQumP80RgBSBKI9LG1vo95OEfsD/wK5IT9BXLc93ewGTgN8CBwNikvU/Ja3sDU4H19doXuB44p9Z2eYi7at2ZwOJkerecAtwLHJunuIGllAqW54APdKO9e9Xlkn75U1uTZbkmaSpwFLAamBgR2wCS1wnZRTai64CLgHeT+fHAaxGxM5nPY7t/EHgJuCnpKrpB0vvIeXtHxG+BayhVW9uAHcA68t/eqZHat5/+Vv8BuCeZznXckmYDv42IX1at6mjcvUroqrEs19dLStoP+AFwfkS8nnU89Ug6HdgeEesqF9fYNG/tPhb4KPCdiDiK0r1+ctW9UkvS5zwHOBT4U+B9lA6fq+Wtvevph+8Mki6j1D26OF1UY7NcxC1pX+Ay4PJaq2ssaznuXiX0rcAhFfNTgBd6tO+mSdqLUjJfHBE/TBa/KGlSsn4SsD2r+EYwA5gt6TlgCaVul+uAAySNTbbJY7tvBbZGxOpkfimlBJ/39v4k8KuIeCki/gj8EDiO/Ld3aqT2zf3fqqS5wOnAZyPppyDfcf85pf/x/zL5+5wCPCrpYDocd68S+hpgWnIFwN6UTl4s79G+myJJwI3Axoj4ZsWq5cDcZHoupb713IiISyJiSkRMpdS+90fEZ4EHgE8nm+Ux7v8FfiNpMFn0CeBJct7elLpajpG0b/KdSePOdXtXGKl9lwPnJVdfHAPsSLtm8kDSLOBiYHZEvF2xajlwtqR9JB1K6STjz7OIsVpEPBEREyJiavL3uRX4aPLd72x79/AkwamUzko/C1yW1cmKBuL8a0qHPI8Dv0j+nUqpP/o+YHPyemDWsY7yGWYCK5LpD1L6Yj8D/DewT9bx1Yj3SGBt0uZ3AuP6ob2BK4CngPXArcA+eWxv4DZK/fx/TJLJvJHal1IXwLeTv9MnKF3Fk6e4n6HU55z+bX63YvvLkrifBj6Vp7ir1j/HrpOiHW1vD/03MysIjxQ1MysIJ3Qzs4JwQjczKwgndDOzgnBCNzMrCCd0M7OCcEI3MyuI/wf2xELFvRW4QQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Load the image and convert it to grayscale\n",
    "image = cv2.imread('./unsolved-captchas/0.png')\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "# Add some extra padding around the image\n",
    "gray = cv2.copyMakeBorder(gray, 8, 8, 8, 8, cv2.BORDER_REPLICATE)\n",
    "\n",
    "# threshold the image\n",
    "thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV, cv2.THRESH_OTSU)[1]\n",
    "\n",
    "# find the contours\n",
    "contours, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "\n",
    "    \n",
    "letter_image_regions = []\n",
    "\n",
    "# Now we can loop through each of the contours and extract the letter\n",
    "\n",
    "for contour in contours:\n",
    "    # Get the rectangle that contains the contour\n",
    "    (x, y, w, h) = cv2.boundingRect(contour)\n",
    "    \n",
    "    # checking if any counter is too wide\n",
    "    # if countour is too wide then there could be two letters joined together or are very close to each other\n",
    "    if w / h > 1.25:\n",
    "        # Split it in half into two letter regions\n",
    "        half_width = int(w / 2)\n",
    "        letter_image_regions.append((x, y, half_width, h))\n",
    "        letter_image_regions.append((x + half_width, y, half_width, h))\n",
    "    else:\n",
    "        letter_image_regions.append((x, y, w, h))\n",
    "            \n",
    "\n",
    "# Sort the detected letter images based on the x coordinate to make sure\n",
    "# we get them from left-to-right so that we match the right image with the right letter  \n",
    "\n",
    "letter_image_regions = sorted(letter_image_regions, key=lambda x: x[0])\n",
    "\n",
    "# Create an output image and a list to hold our predicted letters\n",
    "output = cv2.merge([gray] * 3)\n",
    "predictions = []\n",
    "    \n",
    "# Creating an empty list for storing predicted letters\n",
    "predictions = []\n",
    "    \n",
    "# Save out each letter as a single image\n",
    "for letter_bounding_box in letter_image_regions:\n",
    "    # Grab the coordinates of the letter in the image\n",
    "    x, y, w, h = letter_bounding_box\n",
    "\n",
    "    # Extract the letter from the original image with a 2-pixel margin around the edge\n",
    "    letter_image = gray[y - 2:y + h + 2, x - 2:x + w + 2]\n",
    "\n",
    "    letter_image = cv2.resize(letter_image, (30,30))\n",
    "        \n",
    "    # Turn the single image into a 4d list of images\n",
    "    letter_image = np.expand_dims(letter_image, axis=2)\n",
    "    letter_image = np.expand_dims(letter_image, axis=0)\n",
    "\n",
    "    # making prediction\n",
    "    pred = model.predict(letter_image)\n",
    "        \n",
    "    # Convert the one-hot-encoded prediction back to a normal letter\n",
    "    letter = lb.inverse_transform(pred)[0]\n",
    "    predictions.append(letter)\n",
    "\n",
    "\n",
    "    # draw the prediction on the output image\n",
    "    cv2.rectangle(output, (x - 2, y - 2), (x + w + 4, y + h + 4), (0, 255, 0), 1)\n",
    "    cv2.putText(output, letter, (x - 5, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.55, (0, 255, 0), 2)\n",
    "\n",
    "# Print the captcha's text\n",
    "captcha_text = \"\".join(predictions)\n",
    "print(\"CAPTCHA text is: {}\".format(captcha_text))\n",
    "\n",
    "# Show the annotated image\n",
    "plt.imshow(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_folder = 'unsolved-captchas'\n",
    "OUTPUT_FOLDER = 'predicted captchas'\n",
    "test_captcha_images = glob.glob(os.path.join(test_image_folder, \"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the image paths\n",
    "for (i, image) in enumerate(test_captcha_images):\n",
    "    print(\"processing image {}/{}\".format(i + 1, len(test_captcha_images)))\n",
    "\n",
    "    # Load the image and convert it to grayscale\n",
    "    img = cv2.imread(image)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Add some extra padding around the image\n",
    "    gray = cv2.copyMakeBorder(gray, 8, 8, 8, 8, cv2.BORDER_REPLICATE)\n",
    "\n",
    "    # threshold the image (convert it to pure black and white)\n",
    "    thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV, cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    # find the contours (continuous blobs of pixels) the image\n",
    "    contours, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    \n",
    "    letter_image_regions = []\n",
    "\n",
    "    # Now we can loop through each of the contours and extract the letter\n",
    "    for contour in contours:\n",
    "        # Get the rectangle that contains the contour\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        \n",
    "        # checking if any counter is too wide\n",
    "        # if countour is too wide then there could be two letters joined together or are very close to each other\n",
    "        if w / h > 1.25:\n",
    "            # Split it in half into two letter regions\n",
    "            half_width = int(w / 2)\n",
    "            letter_image_regions.append((x, y, half_width, h))\n",
    "            letter_image_regions.append((x + half_width, y, half_width, h))\n",
    "        else:\n",
    "            letter_image_regions.append((x, y, w, h))\n",
    "            \n",
    "\n",
    "    \n",
    "    # Sort the detected letter images based on the x coordinate to make sure\n",
    "    # we get them from left-to-right so that we match the right image with the right letter  \n",
    "    letter_image_regions = sorted(letter_image_regions, key=lambda x: x[0])\n",
    "    \n",
    "    # Creating an empty list for storing predicted letters\n",
    "    predictions = []\n",
    "    \n",
    "    # Save out each letter as a single image\n",
    "    for letter_bounding_box in letter_image_regions:\n",
    "        # Grab the coordinates of the letter in the image\n",
    "        x, y, w, h = letter_bounding_box\n",
    "\n",
    "        # Extract the letter from the original image with a 2-pixel margin around the edge\n",
    "        letter_image = gray[y - 2:y + h + 2, x - 2:x + w + 2]\n",
    "\n",
    "        letter_image = cv2.resize(letter_image, (30,30))\n",
    "        \n",
    "        # Turn the single image into a 4d list of images\n",
    "        letter_image = np.expand_dims(letter_image, axis=2)\n",
    "        letter_image = np.expand_dims(letter_image, axis=0)\n",
    "\n",
    "        # making prediction\n",
    "        pred = model.predict(letter_image)\n",
    "        \n",
    "        # Convert the one-hot-encoded prediction back to a normal letter\n",
    "        letter = lb.inverse_transform(pred)[0]\n",
    "        predictions.append(letter)\n",
    "\n",
    "        \n",
    "    # joining predicted captcha's text\n",
    "    captcha_text = \"\".join(predictions)\n",
    "    \n",
    "    # Get the folder to save the image in\n",
    "    save_path = os.path.join(OUTPUT_FOLDER, captcha_text)\n",
    "    \n",
    "    p = os.path.join(save_path+'.png' )\n",
    "    #writing the image to the output folder\n",
    "    cv2.imwrite(p, img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
